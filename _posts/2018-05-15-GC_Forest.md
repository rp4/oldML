---
layout: post
title: "GC Forest"
author: "Rick"
categories: Algorithm
tags: [GC Forest, Machine Learning]
image: accelerometer.jpg
---

Below is an example of a GC Forest model based on Zhou and J. Feng's [documentation](https://arxiv.org/abs/1702.08835) using [MotionSense data](https://www.kaggle.com/malekzadeh/motionsense-dataset) from kaggle. This dataset includes time-series data generated by accelerometer and gyroscope sensors. I will focus on the accelerometer data.


```python
from datetime import datetime
startTime = datetime.now()

import pandas as pd
import numpy as np
import pickle
import matplotlib.pyplot as plt
from scipy import stats
import tensorflow as tf
import seaborn as sns
from pylab import rcParams
import sklearn
from sklearn import metrics
from sklearn.cross_validation import train_test_split
from utils import (checking_na, now, proc_import)
%matplotlib inline
```

# Load Data


```python
#Combine & Label Datasets (excluding downstairs vs upstairs)
download_dir = 'combined.csv'

folders = ['jog_9','jog_16','sit_5','sit_13','std_6'
    ,'std_14','wlk_7','wlk_8','wlk_15']

for folder in folders:
    for num in range(1,25):
        filepath='C:\\Users\\user0\\Desktop\\Projects\\MotionSense\\A_DeviceMotion_data\\A_DeviceMotion_data\\' + folder + '\\sub_' + str(num) + '.csv'
        print(filepath)
        dataset=pd.read_csv(filepath)
        dataset['label']=num
        dataset['subject']=folder[:3]
        with open(download_dir,'a') as f:
            dataset.to_csv(f,header=False)

print(datetime.now()-startTime)
    
```

    C:\Users\user0\Desktop\Projects\MotionSense\A_DeviceMotion_data\A_DeviceMotion_data\jog_9\sub_1.csv
    C:\Users\user0\Desktop\Projects\MotionSense\A_DeviceMotion_data\A_DeviceMotion_data\jog_9\sub_2.csv
    ...
    C:\Users\user0\Desktop\Projects\MotionSense\A_DeviceMotion_data\A_DeviceMotion_data\wlk_15\sub_24.csv
    0:35:29.516010
    


```python
df=pd.read_csv('combined.csv', header=None)
df=df.iloc[:,2:]
```


```python
#Add Header
filepath='C:\\Users\\user0\\Desktop\\Projects\\MotionSense\\A_DeviceMotion_data\\A_DeviceMotion_data\\wlk_15\\sub_1.csv'
example=pd.read_csv(filepath)
col_names = example.columns
col_names = col_names[1:]
added = (['user','activity'])
col_names = np.append(col_names, added)
print(col_names)
```

    ['attitude.roll' 'attitude.pitch' 'attitude.yaw' 'gravity.x' 'gravity.y'
     'gravity.z' 'rotationRate.x' 'rotationRate.y' 'rotationRate.z'
     'userAcceleration.x' 'userAcceleration.y' 'userAcceleration.z' 'user'
     'activity']
    


```python
df.columns = col_names
df.head()
```




<div>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>attitude.roll</th>
      <th>attitude.pitch</th>
      <th>attitude.yaw</th>
      <th>gravity.x</th>
      <th>gravity.y</th>
      <th>gravity.z</th>
      <th>rotationRate.x</th>
      <th>rotationRate.y</th>
      <th>rotationRate.z</th>
      <th>userAcceleration.x</th>
      <th>userAcceleration.y</th>
      <th>userAcceleration.z</th>
      <th>user</th>
      <th>activity</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>3.058304</td>
      <td>-1.227988</td>
      <td>2.570999</td>
      <td>0.027964</td>
      <td>0.941814</td>
      <td>0.334969</td>
      <td>0.160508</td>
      <td>-1.386834</td>
      <td>-0.749713</td>
      <td>0.204199</td>
      <td>0.172657</td>
      <td>-0.801048</td>
      <td>1</td>
      <td>jog</td>
    </tr>
    <tr>
      <th>1</th>
      <td>3.075964</td>
      <td>-1.225818</td>
      <td>2.615277</td>
      <td>0.022178</td>
      <td>0.941083</td>
      <td>0.337448</td>
      <td>-0.217198</td>
      <td>-0.612402</td>
      <td>-0.682841</td>
      <td>0.089974</td>
      <td>-0.373914</td>
      <td>-0.506332</td>
      <td>1</td>
      <td>jog</td>
    </tr>
    <tr>
      <th>2</th>
      <td>3.103364</td>
      <td>-1.235013</td>
      <td>2.651791</td>
      <td>0.012594</td>
      <td>0.944152</td>
      <td>0.329269</td>
      <td>0.663253</td>
      <td>-0.498534</td>
      <td>-0.620223</td>
      <td>0.260127</td>
      <td>-0.364364</td>
      <td>-0.781249</td>
      <td>1</td>
      <td>jog</td>
    </tr>
    <tr>
      <th>3</th>
      <td>3.109208</td>
      <td>-1.244901</td>
      <td>2.678484</td>
      <td>0.010366</td>
      <td>0.947364</td>
      <td>0.319989</td>
      <td>0.458100</td>
      <td>-1.202168</td>
      <td>-0.304561</td>
      <td>0.584253</td>
      <td>-0.922813</td>
      <td>-0.285169</td>
      <td>1</td>
      <td>jog</td>
    </tr>
    <tr>
      <th>4</th>
      <td>3.074214</td>
      <td>-1.263514</td>
      <td>2.661371</td>
      <td>0.020364</td>
      <td>0.953159</td>
      <td>0.301783</td>
      <td>1.347809</td>
      <td>-0.550578</td>
      <td>0.610944</td>
      <td>0.626501</td>
      <td>-1.045978</td>
      <td>-0.063884</td>
      <td>1</td>
      <td>jog</td>
    </tr>
  </tbody>
</table>
</div>



# EDA


```python
df['activity'].value_counts().plot(kind='bar',
                                   title='Training examples by activity type');
```


<a href="https://github.com/rp4/rp4.github.io/blob/master/assets/img/GC_forest_files/GC_Forest_1.png"><img src="{{ site.github.url }}/assets/img/GC_forest_files/GC_Forest_1.png"></a>


```python
df['user'].value_counts().plot(kind='bar', title='Training examples by user');
```


<a href="https://github.com/rp4/rp4.github.io/blob/master/assets/img/GC_forest_files/GC_Forest_2.png"><img src="{{ site.github.url }}/assets/img/GC_forest_files/GC_Forest_2.png"></a>


```python
def plot_activity(activity, df):
    data = df[df['activity'] == activity][['userAcceleration.x', 
                                           'userAcceleration.y', 'userAcceleration.z']][:200]
    axis = data.plot(subplots=True, figsize=(16, 12), 
                     title=activity)
    for ax in axis:
        ax.legend(loc='lower left', bbox_to_anchor=(1.0, 0.5))
plot_activity("wlk", df)
```

<a href="https://github.com/rp4/rp4.github.io/blob/master/assets/img/GC_forest_files/GC_Forest_3.png"><img src="{{ site.github.url }}/assets/img/GC_forest_files/GC_Forest_3.png"></a>


```python
plot_activity("sit", df)
```

 <a href="https://github.com/rp4/rp4.github.io/blob/master/assets/img/GC_forest_files/GC_Forest_4.png"><img src="{{ site.github.url }}/assets/img/GC_forest_files/GC_Forest_4.png"></a>


```python
plot_activity("std", df)
```


<a href="https://github.com/rp4/rp4.github.io/blob/master/assets/img/GC_forest_files/GC_Forest_5.png"><img src="{{ site.github.url }}/assets/img/GC_forest_files/GC_Forest_5.png"></a>


```python
plot_activity("jog", df)
```


<a href="https://github.com/rp4/rp4.github.io/blob/master/assets/img/GC_forest_files/GC_Forest_6.png"><img src="{{ site.github.url }}/assets/img/GC_forest_files/GC_Forest_6.png"></a>

# Preprocess Data


```python
#Preprocess Data
N_TIME_STEPS = 200
N_FEATURES = 3
step = 20
segments = []
labels = []
for i in range(0, len(df) - N_TIME_STEPS, step):
    xs = df['userAcceleration.x'].values[i: i + N_TIME_STEPS]
    ys = df['userAcceleration.y'].values[i: i + N_TIME_STEPS]
    zs = df['userAcceleration.z'].values[i: i + N_TIME_STEPS]
    label = stats.mode(df['activity'][i: i + N_TIME_STEPS])[0][0]
    segments.append([xs, ys, zs])
    labels.append(label)
np.array(segments).shape

reshaped_segments = np.asarray(segments,
                               dtype= np.float32).reshape(-1, N_TIME_STEPS, N_FEATURES)
labels = sklearn.preprocessing.LabelEncoder().fit_transform(labels)
reshaped_segments.shape

X_train, X_test, y_train, y_test = train_test_split(
        reshaped_segments, labels, test_size=0.1, random_state=7)
```


```python
X_train = X_train[:, np.newaxis, :, :]
X_test = X_test[:, np.newaxis, :, :]
```

# GC Forest


```python
import argparse
import numpy as np
import sys
from keras.datasets import mnist
import pickle
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score
sys.path.insert(0, "lib")
from gcforest.gcforest import GCForest
from gcforest.utils.config_utils import load_json

def parse_args():
    parser = argparse.ArgumentParser()
    parser.add_argument("--model", dest="model", type=str, default=None, help="gcfoest Net Model File")
    args = parser.parse_args()
    return args

def get_toy_config():
    config = {}
    ca_config = {}
    ca_config["random_state"] = 0
    ca_config["max_layers"] = 3
    ca_config["early_stopping_rounds"] = 3
    ca_config["n_classes"] = 4
    ca_config["estimators"] = []
    ca_config["estimators"].append({"n_folds": 5,
                                    "type": "XGBClassifier",
                                    "n_estimators": 10,
                                    "max_depth": 5,
                                    "objective": "multi:softprob", 
                                    "silent": True,
                                    "nthread": -1,
                                    "learning_rate": 0.1} )
    ca_config["estimators"].append({"n_folds": 5,
                                    "type": "RandomForestClassifier",
                                    "n_estimators": 10,
                                    "max_depth": None,
                                    "n_jobs": -1})
    ca_config["estimators"].append({"n_folds": 5,
                                    "type": "ExtraTreesClassifier",
                                    "n_estimators": 10,
                                    "max_depth": None,
                                    "n_jobs": -1})
    ca_config["estimators"].append({"n_folds": 5,
                                    "type": "LogisticRegression"})
    config["cascade"] = ca_config
    return config

```

Set the hyperparameters


```python
#config = load_json('gcforest/demo_mnist-ca.json')
gc = GCForest(get_toy_config())
```


```python
gc.set_keep_model_in_mem(False)
```


```python
X_train_enc = gc.fit_transform(X_train, y_train)
```

    [ 2018-05-16 08:15:55,328][cascade_classifier.fit_transform] X_groups_train.shape=[(50559, 200, 3)],y_train.shape=(50559,),X_groups_test.shape=no_test,y_test.shape=no_test
    [ 2018-05-16 08:15:55,390][cascade_classifier.fit_transform] group_dims=[600]
    [ 2018-05-16 08:15:55,390][cascade_classifier.fit_transform] group_starts=[0]
    [ 2018-05-16 08:15:55,390][cascade_classifier.fit_transform] group_ends=[600]
    [ 2018-05-16 08:15:55,390][cascade_classifier.fit_transform] X_train.shape=(50559, 600),X_test.shape=(0, 600)
    [ 2018-05-16 08:15:55,453][cascade_classifier.fit_transform] [layer=0] look_indexs=[0], X_cur_train.shape=(50559, 600), X_cur_test.shape=(0, 600)
    [ 2018-05-16 08:17:41,854][kfold_wrapper.log_eval_metrics] Accuracy(layer_0 - estimator_0 - 5_folds.train_0.predict)=90.06%
    [ 2018-05-16 08:19:28,694][kfold_wrapper.log_eval_metrics] Accuracy(layer_0 - estimator_0 - 5_folds.train_1.predict)=90.31%
    [ 2018-05-16 08:21:15,283][kfold_wrapper.log_eval_metrics] Accuracy(layer_0 - estimator_0 - 5_folds.train_2.predict)=90.20%
    [ 2018-05-16 08:23:01,592][kfold_wrapper.log_eval_metrics] Accuracy(layer_0 - estimator_0 - 5_folds.train_3.predict)=90.36%
    [ 2018-05-16 08:24:48,119][kfold_wrapper.log_eval_metrics] Accuracy(layer_0 - estimator_0 - 5_folds.train_4.predict)=90.75%
    [ 2018-05-16 08:24:48,134][kfold_wrapper.log_eval_metrics] Accuracy(layer_0 - estimator_0 - 5_folds.train_cv.predict)=90.34%
    [ 2018-05-16 08:24:53,291][kfold_wrapper.log_eval_metrics] Accuracy(layer_0 - estimator_1 - 5_folds.train_0.predict)=95.20%
    [ 2018-05-16 08:24:59,073][kfold_wrapper.log_eval_metrics] Accuracy(layer_0 - estimator_1 - 5_folds.train_1.predict)=95.33%
    [ 2018-05-16 08:25:04,452][kfold_wrapper.log_eval_metrics] Accuracy(layer_0 - estimator_1 - 5_folds.train_2.predict)=95.33%
    [ 2018-05-16 08:25:09,766][kfold_wrapper.log_eval_metrics] Accuracy(layer_0 - estimator_1 - 5_folds.train_3.predict)=95.30%
    [ 2018-05-16 08:25:14,898][kfold_wrapper.log_eval_metrics] Accuracy(layer_0 - estimator_1 - 5_folds.train_4.predict)=95.60%
    [ 2018-05-16 08:25:14,913][kfold_wrapper.log_eval_metrics] Accuracy(layer_0 - estimator_1 - 5_folds.train_cv.predict)=95.35%
    [ 2018-05-16 08:25:17,273][kfold_wrapper.log_eval_metrics] Accuracy(layer_0 - estimator_2 - 5_folds.train_0.predict)=94.82%
    [ 2018-05-16 08:25:19,601][kfold_wrapper.log_eval_metrics] Accuracy(layer_0 - estimator_2 - 5_folds.train_1.predict)=95.51%
    [ 2018-05-16 08:25:21,953][kfold_wrapper.log_eval_metrics] Accuracy(layer_0 - estimator_2 - 5_folds.train_2.predict)=95.73%
    [ 2018-05-16 08:25:24,366][kfold_wrapper.log_eval_metrics] Accuracy(layer_0 - estimator_2 - 5_folds.train_3.predict)=95.37%
    [ 2018-05-16 08:25:26,810][kfold_wrapper.log_eval_metrics] Accuracy(layer_0 - estimator_2 - 5_folds.train_4.predict)=95.59%
    [ 2018-05-16 08:25:26,826][kfold_wrapper.log_eval_metrics] Accuracy(layer_0 - estimator_2 - 5_folds.train_cv.predict)=95.40%
    [ 2018-05-16 08:25:54,485][kfold_wrapper.log_eval_metrics] Accuracy(layer_0 - estimator_3 - 5_folds.train_0.predict)=62.70%
    [ 2018-05-16 08:26:20,300][kfold_wrapper.log_eval_metrics] Accuracy(layer_0 - estimator_3 - 5_folds.train_1.predict)=62.26%
    [ 2018-05-16 08:26:47,892][kfold_wrapper.log_eval_metrics] Accuracy(layer_0 - estimator_3 - 5_folds.train_2.predict)=61.82%
    [ 2018-05-16 08:27:15,223][kfold_wrapper.log_eval_metrics] Accuracy(layer_0 - estimator_3 - 5_folds.train_3.predict)=61.84%
    [ 2018-05-16 08:27:41,195][kfold_wrapper.log_eval_metrics] Accuracy(layer_0 - estimator_3 - 5_folds.train_4.predict)=62.09%
    [ 2018-05-16 08:27:41,211][kfold_wrapper.log_eval_metrics] Accuracy(layer_0 - estimator_3 - 5_folds.train_cv.predict)=62.14%
    [ 2018-05-16 08:27:41,211][cascade_classifier.calc_accuracy] Accuracy(layer_0 - train.classifier_average)=96.19%
    [ 2018-05-16 08:27:41,289][cascade_classifier.fit_transform] [layer=1] look_indexs=[0], X_cur_train.shape=(50559, 616), X_cur_test.shape=(0, 616)
    [ 2018-05-16 08:29:29,547][kfold_wrapper.log_eval_metrics] Accuracy(layer_1 - estimator_0 - 5_folds.train_0.predict)=96.58%
    [ 2018-05-16 08:31:18,518][kfold_wrapper.log_eval_metrics] Accuracy(layer_1 - estimator_0 - 5_folds.train_1.predict)=96.66%
    [ 2018-05-16 08:33:07,092][kfold_wrapper.log_eval_metrics] Accuracy(layer_1 - estimator_0 - 5_folds.train_2.predict)=96.87%
    [ 2018-05-16 08:34:55,130][kfold_wrapper.log_eval_metrics] Accuracy(layer_1 - estimator_0 - 5_folds.train_3.predict)=96.63%
    [ 2018-05-16 08:36:44,859][kfold_wrapper.log_eval_metrics] Accuracy(layer_1 - estimator_0 - 5_folds.train_4.predict)=96.90%
    [ 2018-05-16 08:36:44,859][kfold_wrapper.log_eval_metrics] Accuracy(layer_1 - estimator_0 - 5_folds.train_cv.predict)=96.73%
    [ 2018-05-16 08:36:49,661][kfold_wrapper.log_eval_metrics] Accuracy(layer_1 - estimator_1 - 5_folds.train_0.predict)=96.71%
    [ 2018-05-16 08:36:54,458][kfold_wrapper.log_eval_metrics] Accuracy(layer_1 - estimator_1 - 5_folds.train_1.predict)=96.81%
    [ 2018-05-16 08:36:59,474][kfold_wrapper.log_eval_metrics] Accuracy(layer_1 - estimator_1 - 5_folds.train_2.predict)=96.87%
    [ 2018-05-16 08:37:04,146][kfold_wrapper.log_eval_metrics] Accuracy(layer_1 - estimator_1 - 5_folds.train_3.predict)=96.71%
    [ 2018-05-16 08:37:08,881][kfold_wrapper.log_eval_metrics] Accuracy(layer_1 - estimator_1 - 5_folds.train_4.predict)=96.80%
    [ 2018-05-16 08:37:08,896][kfold_wrapper.log_eval_metrics] Accuracy(layer_1 - estimator_1 - 5_folds.train_cv.predict)=96.78%
    [ 2018-05-16 08:37:10,737][kfold_wrapper.log_eval_metrics] Accuracy(layer_1 - estimator_2 - 5_folds.train_0.predict)=96.75%
    [ 2018-05-16 08:37:12,624][kfold_wrapper.log_eval_metrics] Accuracy(layer_1 - estimator_2 - 5_folds.train_1.predict)=96.77%
    [ 2018-05-16 08:37:14,426][kfold_wrapper.log_eval_metrics] Accuracy(layer_1 - estimator_2 - 5_folds.train_2.predict)=96.78%
    [ 2018-05-16 08:37:16,254][kfold_wrapper.log_eval_metrics] Accuracy(layer_1 - estimator_2 - 5_folds.train_3.predict)=96.81%
    [ 2018-05-16 08:37:18,005][kfold_wrapper.log_eval_metrics] Accuracy(layer_1 - estimator_2 - 5_folds.train_4.predict)=96.88%
    [ 2018-05-16 08:37:18,020][kfold_wrapper.log_eval_metrics] Accuracy(layer_1 - estimator_2 - 5_folds.train_cv.predict)=96.80%
    [ 2018-05-16 08:37:53,555][kfold_wrapper.log_eval_metrics] Accuracy(layer_1 - estimator_3 - 5_folds.train_0.predict)=96.08%
    [ 2018-05-16 08:38:28,903][kfold_wrapper.log_eval_metrics] Accuracy(layer_1 - estimator_3 - 5_folds.train_1.predict)=96.41%
    [ 2018-05-16 08:39:03,120][kfold_wrapper.log_eval_metrics] Accuracy(layer_1 - estimator_3 - 5_folds.train_2.predict)=96.21%
    [ 2018-05-16 08:39:37,092][kfold_wrapper.log_eval_metrics] Accuracy(layer_1 - estimator_3 - 5_folds.train_3.predict)=96.45%
    [ 2018-05-16 08:40:10,908][kfold_wrapper.log_eval_metrics] Accuracy(layer_1 - estimator_3 - 5_folds.train_4.predict)=96.39%
    [ 2018-05-16 08:40:10,924][kfold_wrapper.log_eval_metrics] Accuracy(layer_1 - estimator_3 - 5_folds.train_cv.predict)=96.31%
    [ 2018-05-16 08:40:10,924][cascade_classifier.calc_accuracy] Accuracy(layer_1 - train.classifier_average)=97.01%
    [ 2018-05-16 08:40:11,033][cascade_classifier.fit_transform] [layer=2] look_indexs=[0], X_cur_train.shape=(50559, 616), X_cur_test.shape=(0, 616)
    [ 2018-05-16 08:41:57,654][kfold_wrapper.log_eval_metrics] Accuracy(layer_2 - estimator_0 - 5_folds.train_0.predict)=97.26%
    [ 2018-05-16 08:43:46,838][kfold_wrapper.log_eval_metrics] Accuracy(layer_2 - estimator_0 - 5_folds.train_1.predict)=97.14%
    [ 2018-05-16 08:45:33,443][kfold_wrapper.log_eval_metrics] Accuracy(layer_2 - estimator_0 - 5_folds.train_2.predict)=97.21%
    [ 2018-05-16 08:47:19,658][kfold_wrapper.log_eval_metrics] Accuracy(layer_2 - estimator_0 - 5_folds.train_3.predict)=97.48%
    [ 2018-05-16 08:49:09,040][kfold_wrapper.log_eval_metrics] Accuracy(layer_2 - estimator_0 - 5_folds.train_4.predict)=97.24%
    [ 2018-05-16 08:49:09,056][kfold_wrapper.log_eval_metrics] Accuracy(layer_2 - estimator_0 - 5_folds.train_cv.predict)=97.27%
    [ 2018-05-16 08:49:13,409][kfold_wrapper.log_eval_metrics] Accuracy(layer_2 - estimator_1 - 5_folds.train_0.predict)=96.83%
    [ 2018-05-16 08:49:17,572][kfold_wrapper.log_eval_metrics] Accuracy(layer_2 - estimator_1 - 5_folds.train_1.predict)=96.89%
    [ 2018-05-16 08:49:22,047][kfold_wrapper.log_eval_metrics] Accuracy(layer_2 - estimator_1 - 5_folds.train_2.predict)=96.98%
    [ 2018-05-16 08:49:26,427][kfold_wrapper.log_eval_metrics] Accuracy(layer_2 - estimator_1 - 5_folds.train_3.predict)=97.21%
    [ 2018-05-16 08:49:30,677][kfold_wrapper.log_eval_metrics] Accuracy(layer_2 - estimator_1 - 5_folds.train_4.predict)=97.13%
    [ 2018-05-16 08:49:30,693][kfold_wrapper.log_eval_metrics] Accuracy(layer_2 - estimator_1 - 5_folds.train_cv.predict)=97.01%
    [ 2018-05-16 08:49:32,517][kfold_wrapper.log_eval_metrics] Accuracy(layer_2 - estimator_2 - 5_folds.train_0.predict)=97.11%
    [ 2018-05-16 08:49:34,417][kfold_wrapper.log_eval_metrics] Accuracy(layer_2 - estimator_2 - 5_folds.train_1.predict)=97.10%
    [ 2018-05-16 08:49:36,393][kfold_wrapper.log_eval_metrics] Accuracy(layer_2 - estimator_2 - 5_folds.train_2.predict)=97.30%
    [ 2018-05-16 08:49:38,299][kfold_wrapper.log_eval_metrics] Accuracy(layer_2 - estimator_2 - 5_folds.train_3.predict)=97.11%
    [ 2018-05-16 08:49:40,170][kfold_wrapper.log_eval_metrics] Accuracy(layer_2 - estimator_2 - 5_folds.train_4.predict)=97.14%
    [ 2018-05-16 08:49:40,186][kfold_wrapper.log_eval_metrics] Accuracy(layer_2 - estimator_2 - 5_folds.train_cv.predict)=97.15%
    [ 2018-05-16 08:50:11,470][kfold_wrapper.log_eval_metrics] Accuracy(layer_2 - estimator_3 - 5_folds.train_0.predict)=97.00%
    [ 2018-05-16 08:50:43,271][kfold_wrapper.log_eval_metrics] Accuracy(layer_2 - estimator_3 - 5_folds.train_1.predict)=97.10%
    [ 2018-05-16 08:51:13,711][kfold_wrapper.log_eval_metrics] Accuracy(layer_2 - estimator_3 - 5_folds.train_2.predict)=96.94%
    [ 2018-05-16 08:51:44,965][kfold_wrapper.log_eval_metrics] Accuracy(layer_2 - estimator_3 - 5_folds.train_3.predict)=97.20%
    [ 2018-05-16 08:52:17,124][kfold_wrapper.log_eval_metrics] Accuracy(layer_2 - estimator_3 - 5_folds.train_4.predict)=97.19%
    [ 2018-05-16 08:52:17,124][kfold_wrapper.log_eval_metrics] Accuracy(layer_2 - estimator_3 - 5_folds.train_cv.predict)=97.09%
    [ 2018-05-16 08:52:17,124][cascade_classifier.calc_accuracy] Accuracy(layer_2 - train.classifier_average)=97.32%
    [ 2018-05-16 08:52:17,156][cascade_classifier.fit_transform] [Result][Reach Max Layer] opt_layer_num=3, accuracy_train=97.32%, accuracy_test=0.00%
    

As seen above, the model reached above 97% accuracy within 3 layers. I manually stopped it after three as the next couple layers didn't add significant lift. Nevertheless, the model ran much faster than the LSTM RNN I ran on the same dataset.


```python
# You can try passing X_enc to another classfier on top of gcForest.e.g. xgboost/RF.
X_test_enc = gc.transform(X_test)
X_train_enc = X_train_enc.reshape((X_train_enc.shape[0], -1))
X_test_enc = X_test_enc.reshape((X_test_enc.shape[0], -1))
X_train_origin = X_train.reshape((X_train.shape[0], -1))
X_test_origin = X_test.reshape((X_test.shape[0], -1))
X_train_enc = np.hstack((X_train_origin, X_train_enc))
X_test_enc = np.hstack((X_test_origin, X_test_enc))
print("X_train_enc.shape={}, X_test_enc.shape={}".format(X_train_enc.shape, X_test_enc.shape))
clf = RandomForestClassifier(n_estimators=1000, max_depth=None, n_jobs=-1)
clf.fit(X_train_enc, y_train)
y_pred = clf.predict(X_test_enc)
acc = accuracy_score(y_test, y_pred)

print("Test Accuracy of Other classifier using gcforest's X_encode = {:.2f} %".format(acc * 100))
```


```python
# SAVE MODEL
with open("test.pkl", "wb") as f:
    pickle.dump(gc, f, pickle.HIGHEST_PROTOCOL)
# LOAD MODEL
with open("test.pkl", "rb") as f:
    gc = pickle.load(f)
y_pred = gc.predict(X_test)
acc = accuracy_score(y_test, y_pred)
print("Test Accuracy of GcForest (save and load) = {:.2f} %".format(acc * 100))
```

    [ 2018-05-16 08:52:25,462][cascade_classifier.transform] X_groups_test.shape=[(5618, 200, 3)]
    [ 2018-05-16 08:52:25,650][cascade_classifier.transform] group_dims=[600]
    [ 2018-05-16 08:52:25,650][cascade_classifier.transform] X_test.shape=(5618, 600)
    [ 2018-05-16 08:52:25,665][cascade_classifier.transform] [layer=0] look_indexs=[0], X_cur_test.shape=(5618, 600)
    [ 2018-05-16 08:52:27,400][cascade_classifier.transform] [layer=1] look_indexs=[0], X_cur_test.shape=(5618, 616)
    [ 2018-05-16 08:52:29,072][cascade_classifier.transform] [layer=2] look_indexs=[0], X_cur_test.shape=(5618, 616)
    

    Test Accuracy of GcForest (save and load) = 98.34 %
    

#### Confusion Matrix


```python
#Confusion Matrix
LABELS = ['Jogging', 'Sitting', 'Standing', 'Walking']
confusion_matrix = metrics.confusion_matrix(y_test, y_pred)
plt.figure(figsize=(16, 14))
sns.heatmap(confusion_matrix, xticklabels=LABELS, yticklabels=LABELS, annot=True, fmt="d",cmap="YlGnBu",cbar=False);
plt.title("Confusion matrix")
plt.ylabel('True label')
plt.xlabel('Predicted label')
plt.show();
```


 <a href="https://github.com/rp4/rp4.github.io/blob/master/assets/img/GC_forest_files/GC_Forest_7.png"><img src="{{ site.github.url }}/assets/img/GC_forest_files/GC_Forest_7.png"></a>

#### Precision and Recall


```python
#PRECISION AND RECALL
report = sklearn.metrics.classification_report( y_test, y_pred )
print(report)
```

                 precision    recall  f1-score   support
    
              0       1.00      0.99      0.99       676
              1       0.97      0.98      0.98      1708
              2       0.98      0.97      0.97      1590
              3       1.00      1.00      1.00      1644
    
    avg / total       0.98      0.98      0.98      5618
    
    

Overall, The GC Forest had slightly better accuracy, and took less time to train than the LTSM model run on the same data.

# Reduced Training Sample
The authors of GC forest claim that it would require less data to train than a deep neural network. To test this hypothesis, I reran the above code with only 20% in the Training data.


```python
X_train, X_test, y_train, y_test = train_test_split(
        reshaped_segments, labels, test_size=0.8, random_state=7)
```

The results trained with only 20% were comparable to the results trained with 90%.

             precision    recall  f1-score   support

          0       1.00      0.99      0.99      5306
          1       0.96      0.96      0.96     13594
          2       0.95      0.96      0.95     12217
          3       0.99      1.00      1.00     13825

    avg / total       0.97      0.97      0.97     44942

Unlike the results for the [LSTM model](https://github.com/rp4/rp4.github.io/blob/master/_posts/2015-02-20-LSTM.md), which had issues identifying sitting(2) vs standing(3) using only 20% of the data in the training set. 

             precision    recall  f1-score   support

          0       1.00      0.99      0.99      5306
          1       0.68      0.80      0.73     13594
          2       0.72      0.58      0.64     12217
          3       1.00      1.00      1.00     13825

    avg / total       0.82      0.82      0.82     44942

<a href="https://github.com/rp4/rp4.github.io/blob/master/assets/img/GC_forest_files/GC_Forest_8.png"><img src="{{ site.github.url }}/assets/img/GC_forest_files/GC_Forest_8.png"></a>

In conclusion, the GC-Forest outperformed the [LSTM model](https://github.com/rp4/rp4.github.io/blob/master/_posts/2015-02-20-LSTM.md) on this [dataset](https://www.kaggle.com/malekzadeh/motionsense-dataset) both using 90% or 20% as training data.


```python

```
